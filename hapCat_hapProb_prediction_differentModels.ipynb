{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6dc85868",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyreadr\n",
      "  Downloading pyreadr-0.4.7-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (364 kB)\n",
      "     |████████████████████████████████| 364 kB 22.8 MB/s            \n",
      "\u001b[?25hCollecting lightgbm\n",
      "  Downloading lightgbm-3.3.5-py3-none-manylinux1_x86_64.whl (2.0 MB)\n",
      "     |████████████████████████████████| 2.0 MB 64.7 MB/s            \n",
      "\u001b[?25hRequirement already satisfied: pandas>=1.2.0 in /opt/conda/lib/python3.8/site-packages (from pyreadr) (1.3.1)\n",
      "Requirement already satisfied: scikit-learn!=0.22.0 in /opt/conda/lib/python3.8/site-packages (from lightgbm) (0.24.0)\n",
      "Requirement already satisfied: scipy in /opt/conda/lib/python3.8/site-packages (from lightgbm) (1.6.0)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.8/site-packages (from lightgbm) (1.21.1)\n",
      "Requirement already satisfied: wheel in /opt/conda/lib/python3.8/site-packages (from lightgbm) (0.36.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.8/site-packages (from pandas>=1.2.0->pyreadr) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.8/site-packages (from pandas>=1.2.0->pyreadr) (2021.3)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.8/site-packages (from scikit-learn!=0.22.0->lightgbm) (2.1.0)\n",
      "Requirement already satisfied: joblib>=0.11 in /opt/conda/lib/python3.8/site-packages (from scikit-learn!=0.22.0->lightgbm) (1.0.0)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.8/site-packages (from python-dateutil>=2.7.3->pandas>=1.2.0->pyreadr) (1.15.0)\n",
      "Installing collected packages: pyreadr, lightgbm\n",
      "Successfully installed lightgbm-3.3.5 pyreadr-0.4.7\n",
      "\u001b[33mWARNING: You are using pip version 21.3.1; however, version 23.1.2 is available.\n",
      "You should consider upgrading via the '/opt/conda/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "#install pyreadr on terminal\n",
    "! pip install pyreadr lightgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "09fb7310",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from termcolor import colored\n",
    "\n",
    "from sklearn.preprocessing import PolynomialFeatures, OneHotEncoder, StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, r2_score, silhouette_score\n",
    "from sklearn.metrics import confusion_matrix,precision_recall_curve, roc_curve\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score,accuracy_score, roc_auc_score\n",
    "\n",
    "from sklearn.model_selection import train_test_split, StratifiedShuffleSplit\n",
    "from sklearn.model_selection import cross_val_score, cross_val_predict\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.linear_model import SGDRegressor, SGDClassifier\n",
    "from sklearn.linear_model import Ridge,Lasso,ElasticNet\n",
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor, BaggingClassifier \n",
    "from sklearn.ensemble import BaggingClassifier, GradientBoostingRegressor\n",
    "from sklearn.svm import SVC,SVR\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.decomposition import PCA, IncrementalPCA, KernelPCA\n",
    "from sklearn.cluster import KMeans, MiniBatchKMeans, DBSCAN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3003c8f",
   "metadata": {},
   "source": [
    "# Haplotype model - Categorical variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8bca64b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyreadr\n",
    "\n",
    "# read in female data\n",
    "result = pyreadr.read_r('/mnt/ML_HBLUP/NA_RM105_110_115/data/dummyMatrix_female.rds') # also works for RData\n",
    "# done! \n",
    "# result is a dictionary where keys are the name of objects and the values python\n",
    "# objects. In the case of Rds there is only one object with None as key\n",
    "femaleData = result[None] # extract the pandas data frame \n",
    "\n",
    "# read in male data\n",
    "result = pyreadr.read_r('/mnt/ML_HBLUP/NA_RM105_110_115/data/dummyMatrix_male.rds') # also works for RData\n",
    "maleData = result[None] # extract the pandas data frame "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ffdf4c7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>HB1__1067-1</th>\n",
       "      <th>HB1__32843</th>\n",
       "      <th>HB1__64DWA2</th>\n",
       "      <th>HB1__B73</th>\n",
       "      <th>HB1__MANS</th>\n",
       "      <th>HB1__NA</th>\n",
       "      <th>HB1__WDAQ2</th>\n",
       "      <th>HB2__1067-1</th>\n",
       "      <th>HB2__32843</th>\n",
       "      <th>HB2__64DWA2</th>\n",
       "      <th>...</th>\n",
       "      <th>HB17115__7797</th>\n",
       "      <th>HB17115__B73</th>\n",
       "      <th>HB17115__NA</th>\n",
       "      <th>HB17115__RQAA8</th>\n",
       "      <th>HB17116__2FACC</th>\n",
       "      <th>HB17116__7797</th>\n",
       "      <th>HB17116__B73</th>\n",
       "      <th>HB17116__FBMU</th>\n",
       "      <th>HB17116__NA</th>\n",
       "      <th>HB17116__RQAA8</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>01DHD10</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>01DHD16</th>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>01DKD2-BGL-T1A1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>01DKD2-NQR-T1B1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>01DKD2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 54753 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 HB1__1067-1  HB1__32843  HB1__64DWA2  HB1__B73  HB1__MANS  \\\n",
       "01DHD10                  0.0         0.0          0.0       2.0        0.0   \n",
       "01DHD16                  2.0         0.0          0.0       0.0        0.0   \n",
       "01DKD2-BGL-T1A1          0.0         0.0          0.0       0.0        0.0   \n",
       "01DKD2-NQR-T1B1          0.0         0.0          0.0       0.0        0.0   \n",
       "01DKD2                   0.0         0.0          0.0       2.0        0.0   \n",
       "\n",
       "                 HB1__NA  HB1__WDAQ2  HB2__1067-1  HB2__32843  HB2__64DWA2  \\\n",
       "01DHD10              0.0         0.0          0.0         0.0          0.0   \n",
       "01DHD16              0.0         0.0          2.0         0.0          0.0   \n",
       "01DKD2-BGL-T1A1      2.0         0.0          0.0         0.0          0.0   \n",
       "01DKD2-NQR-T1B1      2.0         0.0          0.0         0.0          0.0   \n",
       "01DKD2               0.0         0.0          0.0         0.0          0.0   \n",
       "\n",
       "                 ...  HB17115__7797  HB17115__B73  HB17115__NA  \\\n",
       "01DHD10          ...            0.0           0.0          2.0   \n",
       "01DHD16          ...            0.0           0.0          2.0   \n",
       "01DKD2-BGL-T1A1  ...            0.0           2.0          0.0   \n",
       "01DKD2-NQR-T1B1  ...            0.0           2.0          0.0   \n",
       "01DKD2           ...            0.0           2.0          0.0   \n",
       "\n",
       "                 HB17115__RQAA8  HB17116__2FACC  HB17116__7797  HB17116__B73  \\\n",
       "01DHD10                     0.0             0.0            0.0           0.0   \n",
       "01DHD16                     0.0             0.0            0.0           0.0   \n",
       "01DKD2-BGL-T1A1             0.0             0.0            0.0           2.0   \n",
       "01DKD2-NQR-T1B1             0.0             0.0            0.0           2.0   \n",
       "01DKD2                      0.0             0.0            0.0           2.0   \n",
       "\n",
       "                 HB17116__FBMU  HB17116__NA  HB17116__RQAA8  \n",
       "01DHD10                    0.0          2.0             0.0  \n",
       "01DHD16                    0.0          2.0             0.0  \n",
       "01DKD2-BGL-T1A1            0.0          0.0             0.0  \n",
       "01DKD2-NQR-T1B1            0.0          0.0             0.0  \n",
       "01DKD2                     0.0          0.0             0.0  \n",
       "\n",
       "[5 rows x 54753 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "femaleData.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5503f2eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>HB1__01HGI4</th>\n",
       "      <th>HB1__610</th>\n",
       "      <th>HB1__B14</th>\n",
       "      <th>HB1__LH287</th>\n",
       "      <th>HB1__M3AG-3</th>\n",
       "      <th>HB1__NA</th>\n",
       "      <th>HB1__OH43AE1</th>\n",
       "      <th>HB1__PH207</th>\n",
       "      <th>HB2__610</th>\n",
       "      <th>HB2__B14</th>\n",
       "      <th>...</th>\n",
       "      <th>HB17114__OH07</th>\n",
       "      <th>HB17114__PH207</th>\n",
       "      <th>HB17115__LH123</th>\n",
       "      <th>HB17115__NA</th>\n",
       "      <th>HB17115__OH07</th>\n",
       "      <th>HB17115__TA1180</th>\n",
       "      <th>HB17116__LH123</th>\n",
       "      <th>HB17116__NA</th>\n",
       "      <th>HB17116__OH07</th>\n",
       "      <th>HB17116__PH207</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>LH287</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83INI14</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17IFI6</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DILU757</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GEJO564</th>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 54845 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         HB1__01HGI4  HB1__610  HB1__B14  HB1__LH287  HB1__M3AG-3  HB1__NA  \\\n",
       "LH287            0.0       0.0       0.0         2.0          0.0      0.0   \n",
       "83INI14          0.0       0.0       0.0         0.0          0.0      0.0   \n",
       "17IFI6           0.0       0.0       0.0         0.0          0.0      0.0   \n",
       "DILU757          0.0       0.0       0.0         0.0          0.0      0.0   \n",
       "GEJO564          2.0       0.0       0.0         0.0          0.0      0.0   \n",
       "\n",
       "         HB1__OH43AE1  HB1__PH207  HB2__610  HB2__B14  ...  HB17114__OH07  \\\n",
       "LH287             0.0         0.0       0.0       0.0  ...            0.0   \n",
       "83INI14           0.0         2.0       0.0       0.0  ...            0.0   \n",
       "17IFI6            0.0         2.0       0.0       0.0  ...            0.0   \n",
       "DILU757           0.0         2.0       0.0       0.0  ...            0.0   \n",
       "GEJO564           0.0         0.0       0.0       0.0  ...            0.0   \n",
       "\n",
       "         HB17114__PH207  HB17115__LH123  HB17115__NA  HB17115__OH07  \\\n",
       "LH287               0.0             0.0          2.0            0.0   \n",
       "83INI14             2.0             0.0          0.0            0.0   \n",
       "17IFI6              2.0             0.0          0.0            0.0   \n",
       "DILU757             0.0             0.0          0.0            0.0   \n",
       "GEJO564             0.0             2.0          0.0            0.0   \n",
       "\n",
       "         HB17115__TA1180  HB17116__LH123  HB17116__NA  HB17116__OH07  \\\n",
       "LH287                0.0             0.0          2.0            0.0   \n",
       "83INI14              0.0             0.0          0.0            0.0   \n",
       "17IFI6               0.0             0.0          0.0            0.0   \n",
       "DILU757              0.0             0.0          0.0            0.0   \n",
       "GEJO564              0.0             2.0          0.0            0.0   \n",
       "\n",
       "         HB17116__PH207  \n",
       "LH287               0.0  \n",
       "83INI14             2.0  \n",
       "17IFI6              2.0  \n",
       "DILU757             0.0  \n",
       "GEJO564             0.0  \n",
       "\n",
       "[5 rows x 54845 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "maleData.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "17eab1c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add suffix\n",
    "femaleData.columns += '_f'\n",
    "maleData.columns += '_m'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "71eb3f47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>HB1__01HGI4_m</th>\n",
       "      <th>HB1__610_m</th>\n",
       "      <th>HB1__B14_m</th>\n",
       "      <th>HB1__LH287_m</th>\n",
       "      <th>HB1__M3AG-3_m</th>\n",
       "      <th>HB1__NA_m</th>\n",
       "      <th>HB1__OH43AE1_m</th>\n",
       "      <th>HB1__PH207_m</th>\n",
       "      <th>HB2__610_m</th>\n",
       "      <th>HB2__B14_m</th>\n",
       "      <th>...</th>\n",
       "      <th>HB17114__OH07_m</th>\n",
       "      <th>HB17114__PH207_m</th>\n",
       "      <th>HB17115__LH123_m</th>\n",
       "      <th>HB17115__NA_m</th>\n",
       "      <th>HB17115__OH07_m</th>\n",
       "      <th>HB17115__TA1180_m</th>\n",
       "      <th>HB17116__LH123_m</th>\n",
       "      <th>HB17116__NA_m</th>\n",
       "      <th>HB17116__OH07_m</th>\n",
       "      <th>HB17116__PH207_m</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>LH287</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83INI14</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17IFI6</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DILU757</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GEJO564</th>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 54845 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         HB1__01HGI4_m  HB1__610_m  HB1__B14_m  HB1__LH287_m  HB1__M3AG-3_m  \\\n",
       "LH287              0.0         0.0         0.0           2.0            0.0   \n",
       "83INI14            0.0         0.0         0.0           0.0            0.0   \n",
       "17IFI6             0.0         0.0         0.0           0.0            0.0   \n",
       "DILU757            0.0         0.0         0.0           0.0            0.0   \n",
       "GEJO564            2.0         0.0         0.0           0.0            0.0   \n",
       "\n",
       "         HB1__NA_m  HB1__OH43AE1_m  HB1__PH207_m  HB2__610_m  HB2__B14_m  ...  \\\n",
       "LH287          0.0             0.0           0.0         0.0         0.0  ...   \n",
       "83INI14        0.0             0.0           2.0         0.0         0.0  ...   \n",
       "17IFI6         0.0             0.0           2.0         0.0         0.0  ...   \n",
       "DILU757        0.0             0.0           2.0         0.0         0.0  ...   \n",
       "GEJO564        0.0             0.0           0.0         0.0         0.0  ...   \n",
       "\n",
       "         HB17114__OH07_m  HB17114__PH207_m  HB17115__LH123_m  HB17115__NA_m  \\\n",
       "LH287                0.0               0.0               0.0            2.0   \n",
       "83INI14              0.0               2.0               0.0            0.0   \n",
       "17IFI6               0.0               2.0               0.0            0.0   \n",
       "DILU757              0.0               0.0               0.0            0.0   \n",
       "GEJO564              0.0               0.0               2.0            0.0   \n",
       "\n",
       "         HB17115__OH07_m  HB17115__TA1180_m  HB17116__LH123_m  HB17116__NA_m  \\\n",
       "LH287                0.0                0.0               0.0            2.0   \n",
       "83INI14              0.0                0.0               0.0            0.0   \n",
       "17IFI6               0.0                0.0               0.0            0.0   \n",
       "DILU757              0.0                0.0               0.0            0.0   \n",
       "GEJO564              0.0                0.0               2.0            0.0   \n",
       "\n",
       "         HB17116__OH07_m  HB17116__PH207_m  \n",
       "LH287                0.0               0.0  \n",
       "83INI14              0.0               2.0  \n",
       "17IFI6               0.0               2.0  \n",
       "DILU757              0.0               0.0  \n",
       "GEJO564              0.0               0.0  \n",
       "\n",
       "[5 rows x 54845 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "maleData.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9ad03751",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>HB1__1067-1_f</th>\n",
       "      <th>HB1__32843_f</th>\n",
       "      <th>HB1__64DWA2_f</th>\n",
       "      <th>HB1__B73_f</th>\n",
       "      <th>HB1__MANS_f</th>\n",
       "      <th>HB1__NA_f</th>\n",
       "      <th>HB1__WDAQ2_f</th>\n",
       "      <th>HB2__1067-1_f</th>\n",
       "      <th>HB2__32843_f</th>\n",
       "      <th>HB2__64DWA2_f</th>\n",
       "      <th>...</th>\n",
       "      <th>HB17115__7797_f</th>\n",
       "      <th>HB17115__B73_f</th>\n",
       "      <th>HB17115__NA_f</th>\n",
       "      <th>HB17115__RQAA8_f</th>\n",
       "      <th>HB17116__2FACC_f</th>\n",
       "      <th>HB17116__7797_f</th>\n",
       "      <th>HB17116__B73_f</th>\n",
       "      <th>HB17116__FBMU_f</th>\n",
       "      <th>HB17116__NA_f</th>\n",
       "      <th>HB17116__RQAA8_f</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>01DHD10</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>01DHD16</th>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>01DKD2-BGL-T1A1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>01DKD2-NQR-T1B1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>01DKD2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 54753 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 HB1__1067-1_f  HB1__32843_f  HB1__64DWA2_f  HB1__B73_f  \\\n",
       "01DHD10                    0.0           0.0            0.0         2.0   \n",
       "01DHD16                    2.0           0.0            0.0         0.0   \n",
       "01DKD2-BGL-T1A1            0.0           0.0            0.0         0.0   \n",
       "01DKD2-NQR-T1B1            0.0           0.0            0.0         0.0   \n",
       "01DKD2                     0.0           0.0            0.0         2.0   \n",
       "\n",
       "                 HB1__MANS_f  HB1__NA_f  HB1__WDAQ2_f  HB2__1067-1_f  \\\n",
       "01DHD10                  0.0        0.0           0.0            0.0   \n",
       "01DHD16                  0.0        0.0           0.0            2.0   \n",
       "01DKD2-BGL-T1A1          0.0        2.0           0.0            0.0   \n",
       "01DKD2-NQR-T1B1          0.0        2.0           0.0            0.0   \n",
       "01DKD2                   0.0        0.0           0.0            0.0   \n",
       "\n",
       "                 HB2__32843_f  HB2__64DWA2_f  ...  HB17115__7797_f  \\\n",
       "01DHD10                   0.0            0.0  ...              0.0   \n",
       "01DHD16                   0.0            0.0  ...              0.0   \n",
       "01DKD2-BGL-T1A1           0.0            0.0  ...              0.0   \n",
       "01DKD2-NQR-T1B1           0.0            0.0  ...              0.0   \n",
       "01DKD2                    0.0            0.0  ...              0.0   \n",
       "\n",
       "                 HB17115__B73_f  HB17115__NA_f  HB17115__RQAA8_f  \\\n",
       "01DHD10                     0.0            2.0               0.0   \n",
       "01DHD16                     0.0            2.0               0.0   \n",
       "01DKD2-BGL-T1A1             2.0            0.0               0.0   \n",
       "01DKD2-NQR-T1B1             2.0            0.0               0.0   \n",
       "01DKD2                      2.0            0.0               0.0   \n",
       "\n",
       "                 HB17116__2FACC_f  HB17116__7797_f  HB17116__B73_f  \\\n",
       "01DHD10                       0.0              0.0             0.0   \n",
       "01DHD16                       0.0              0.0             0.0   \n",
       "01DKD2-BGL-T1A1               0.0              0.0             2.0   \n",
       "01DKD2-NQR-T1B1               0.0              0.0             2.0   \n",
       "01DKD2                        0.0              0.0             2.0   \n",
       "\n",
       "                 HB17116__FBMU_f  HB17116__NA_f  HB17116__RQAA8_f  \n",
       "01DHD10                      0.0            2.0               0.0  \n",
       "01DHD16                      0.0            2.0               0.0  \n",
       "01DKD2-BGL-T1A1              0.0            0.0               0.0  \n",
       "01DKD2-NQR-T1B1              0.0            0.0               0.0  \n",
       "01DKD2                       0.0            0.0               0.0  \n",
       "\n",
       "[5 rows x 54753 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "femaleData.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cf6be371",
   "metadata": {},
   "outputs": [],
   "source": [
    "allHapFemales = femaleData.index\n",
    "allHapMales = maleData.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6ebd0ef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in the train data and test data\n",
    "trainPheno = pd.read_csv('/mnt/ML_HBLUP/NA_RM105_110_115/data/train_phenoData_NA_Corn_hblup_2015-2020_ALL_UDR_105-110-115.csv')\n",
    "testPheno = pd.read_csv('/mnt/ML_HBLUP/NA_RM105_110_115/data/test_phenoData_NA_Corn_hblup_2021_ALL_UDR_105-110-115.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "66f13b00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LINE_NAME</th>\n",
       "      <th>FEMALE</th>\n",
       "      <th>MALE</th>\n",
       "      <th>YLD_BE_BLUP</th>\n",
       "      <th>MST_BE_BLUP</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>01DHD10+LH287</td>\n",
       "      <td>01DHD10</td>\n",
       "      <td>LH287</td>\n",
       "      <td>-15.661</td>\n",
       "      <td>-0.219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>01DHD16+83INI14</td>\n",
       "      <td>01DHD16</td>\n",
       "      <td>83INI14</td>\n",
       "      <td>-41.839</td>\n",
       "      <td>-4.254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>01DKD2-BGL-T1A1+17IFI6</td>\n",
       "      <td>01DKD2-BGL-T1A1</td>\n",
       "      <td>17IFI6</td>\n",
       "      <td>-30.021</td>\n",
       "      <td>-3.251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>01DKD2-BGL-T1A1+DILU757</td>\n",
       "      <td>01DKD2-BGL-T1A1</td>\n",
       "      <td>DILU757</td>\n",
       "      <td>8.279</td>\n",
       "      <td>1.769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>01DKD2-BGL-T1A1+GEJO564</td>\n",
       "      <td>01DKD2-BGL-T1A1</td>\n",
       "      <td>GEJO564</td>\n",
       "      <td>4.171</td>\n",
       "      <td>1.519</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 LINE_NAME           FEMALE     MALE  YLD_BE_BLUP  MST_BE_BLUP\n",
       "0            01DHD10+LH287          01DHD10    LH287      -15.661       -0.219\n",
       "1          01DHD16+83INI14          01DHD16  83INI14      -41.839       -4.254\n",
       "2   01DKD2-BGL-T1A1+17IFI6  01DKD2-BGL-T1A1   17IFI6      -30.021       -3.251\n",
       "3  01DKD2-BGL-T1A1+DILU757  01DKD2-BGL-T1A1  DILU757        8.279        1.769\n",
       "4  01DKD2-BGL-T1A1+GEJO564  01DKD2-BGL-T1A1  GEJO564        4.171        1.519"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainPheno.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5e6e4fd9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>LINE_NAME</th>\n",
       "      <th>FEMALE</th>\n",
       "      <th>MALE</th>\n",
       "      <th>YLD_BE_BLUP</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>JYDB2078+JULI2041</td>\n",
       "      <td>JYDB2078</td>\n",
       "      <td>JULI2041</td>\n",
       "      <td>3.853463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>FIDA1520+GALV1529</td>\n",
       "      <td>FIDA1520</td>\n",
       "      <td>GALV1529</td>\n",
       "      <td>0.083630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>HIQO1923+HILU1759</td>\n",
       "      <td>HIQO1923</td>\n",
       "      <td>HILU1759</td>\n",
       "      <td>-9.287542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>FIDA1520-TCJ-T1A2+HILU672-WQQ-T1A1</td>\n",
       "      <td>FIDA1520-TCJ-T1A2</td>\n",
       "      <td>HILU672-WQQ-T1A1</td>\n",
       "      <td>0.519811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>DEDD1628+HILV1970</td>\n",
       "      <td>DEDD1628</td>\n",
       "      <td>HILV1970</td>\n",
       "      <td>-1.873619</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                           LINE_NAME             FEMALE  \\\n",
       "0           1                   JYDB2078+JULI2041           JYDB2078   \n",
       "1           2                   FIDA1520+GALV1529           FIDA1520   \n",
       "2           3                   HIQO1923+HILU1759           HIQO1923   \n",
       "3           4  FIDA1520-TCJ-T1A2+HILU672-WQQ-T1A1  FIDA1520-TCJ-T1A2   \n",
       "4           5                   DEDD1628+HILV1970           DEDD1628   \n",
       "\n",
       "               MALE  YLD_BE_BLUP  \n",
       "0          JULI2041     3.853463  \n",
       "1          GALV1529     0.083630  \n",
       "2          HILU1759    -9.287542  \n",
       "3  HILU672-WQQ-T1A1     0.519811  \n",
       "4          HILV1970    -1.873619  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testPheno.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "483406f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter trainData by selecting hybrids with both female and male are included in haplotypdata\n",
    "# and Observation is not NA\n",
    "trainPheno = trainPheno[(trainPheno['FEMALE'].isin(allHapFemales)) & trainPheno['MALE'].isin(allHapMales) & trainPheno['YLD_BE_BLUP'].notna()]\n",
    "testPheno = testPheno[testPheno['FEMALE'].isin(allHapFemales) & testPheno['MALE'].isin(allHapMales) & testPheno['YLD_BE_BLUP'].notna()]\n",
    "\n",
    "# drop duplicated rows\n",
    "trainPheno = trainPheno.drop_duplicates()\n",
    "testPheno = testPheno.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "19a414b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(33475, 5)\n",
      "(7307, 5)\n"
     ]
    }
   ],
   "source": [
    "print(trainPheno.shape)\n",
    "print(testPheno.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "27dd4a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct haplotype data for test and train data\n",
    "trainHap = pd.concat([femaleData.loc[trainPheno['FEMALE'],:].reset_index(drop=True),\n",
    "                      maleData.loc[trainPheno['MALE'],:].reset_index(drop=True)],axis=1)\n",
    "trainHap = trainHap / 2\n",
    "\n",
    "testHap = pd.concat([femaleData.loc[testPheno['FEMALE'],:].reset_index(drop=True),\n",
    "                     maleData.loc[testPheno['MALE'],:].reset_index(drop=True)],axis=1)\n",
    "testHap = testHap / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a1ea34a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# splite the train data into train, validationa and test\n",
    "seed = 20230510\n",
    "np.random.seed(seed)\n",
    "X_train, X_test, y_train, y_test = train_test_split(trainHap, trainPheno['YLD_BE_BLUP'], test_size=0.1, random_state=seed)\n",
    "\n",
    "seed = 20230515\n",
    "np.random.seed(seed)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=seed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fe0935ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(33475, 109598)\n",
      "(7307, 109598)\n"
     ]
    }
   ],
   "source": [
    "print(trainHap.shape)\n",
    "print(testHap.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c283d56a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(24101, 109598)\n",
      "(6026, 109598)\n",
      "(3348, 109598)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(X_val.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c29fc75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a function to return correlation and rmse\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "metrics = pd.DataFrame(columns=['Method', 'RMSE_train', 'RMSE_val','RMSE_test','corr_train','corr_val','corr_test'])\n",
    "def pred_rmse_corr(modelRes,modelName,X_train,y_train,X_val,y_val,X_test,y_test,metrics):\n",
    "    pred_train = modelRes.predict(X_train)\n",
    "    pred_val = modelRes.predict(X_val)\n",
    "    pred_test = modelRes.predict(X_test)\n",
    "\n",
    "    rmse_train = mean_squared_error(y_train, pred_train, squared=False)\n",
    "    rmse_val = mean_squared_error(y_val,pred_val, squared=False)\n",
    "    rmse_test = mean_squared_error(y_test, pred_test, squared=False)\n",
    "\n",
    "    corr_train, p_value_train = pearsonr(y_train.ravel(), pred_train.ravel())\n",
    "    corr_val, p_value_val = pearsonr(y_val.ravel(), pred_val.ravel())\n",
    "    corr_test, p_value_test = pearsonr(y_test.ravel(), pred_test.ravel())\n",
    "\n",
    "\n",
    "    metrics_curr_cv = pd.DataFrame(data={'Method': modelName, 'RMSE_train': [rmse_train], 'corr_train': [corr_train],\n",
    "                                     'RMSE_val' : [rmse_val], 'corr_val' : [corr_val], \n",
    "                                     'RMSE_test' : [rmse_test], 'corr_test' : [corr_test]})\n",
    "\n",
    "    metrics = pd.concat([metrics, metrics_curr_cv], axis=0)\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba0bb1b6",
   "metadata": {},
   "source": [
    "## LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ca24c742",
   "metadata": {},
   "outputs": [],
   "source": [
    "# need to install lightgbm from terminal: sudo pip install lightgbm\n",
    "import lightgbm as lgb\n",
    "\n",
    "gbm = lgb.LGBMRegressor(num_leaves=30,\n",
    "                        learning_rate=0.1,\n",
    "                        n_estimators=900,\n",
    "                       bagging_fraction =  0.7,\n",
    "                       feature_fraction = 0.5,\n",
    "                       objective = \"regression\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0d822d7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] feature_fraction is set=0.5, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n",
      "Training until validation scores don't improve for 5 rounds\n"
     ]
    }
   ],
   "source": [
    "gbm.fit(X_train, y_train,\n",
    "        eval_set=[(X_val, y_val)],\n",
    "        eval_metric='mse',\n",
    "        callbacks=[lgb.early_stopping(5)])\n",
    "\n",
    "# save model\n",
    "gbm.booster_.save_model('/mnt/ML_HBLUP/NA_RM105_110_115/models/lgbr_hapCat_30_900.txt')\n",
    "\n",
    "# load model later\n",
    "    #model = lightgbm.Booster(model_file='file.txt')\n",
    "    #model.predict(predict[num_columns])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "51f2af68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the saved model\n",
    "gbm = lgb.Booster(model_file='/mnt/ML_HBLUP/NA_RM105_110_115/models/lgbr_hapCat_30_900.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "97dd50ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Method</th>\n",
       "      <th>RMSE_train</th>\n",
       "      <th>RMSE_val</th>\n",
       "      <th>RMSE_test</th>\n",
       "      <th>corr_train</th>\n",
       "      <th>corr_val</th>\n",
       "      <th>corr_test</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LightGBM_hapCat</td>\n",
       "      <td>2.30542</td>\n",
       "      <td>4.55393</td>\n",
       "      <td>4.440251</td>\n",
       "      <td>0.983027</td>\n",
       "      <td>0.932513</td>\n",
       "      <td>0.933978</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Method  RMSE_train  RMSE_val  RMSE_test  corr_train  corr_val  \\\n",
       "0  LightGBM_hapCat     2.30542   4.55393   4.440251    0.983027  0.932513   \n",
       "\n",
       "   corr_test  \n",
       "0   0.933978  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# output prediction result\n",
    "metrics = pred_rmse_corr(gbm,'LightGBM_hapCat',X_train,y_train,X_val,y_val,X_test,y_test,metrics)\n",
    "metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22279fc2",
   "metadata": {},
   "source": [
    "## Neural network - DNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "85ff17c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbd2a51d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(20230516)\n",
    "\n",
    "DL_model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[X_train.shape[1]], name = 'input_layer'),\n",
    "    keras.layers.Dense(100, activation=\"relu\", name = 'hidden_layer1'),\n",
    "    keras.layers.Dense(300, activation=\"relu\", name = 'hidden_layer2'),\n",
    "    keras.layers.Dense(1, name = 'output_layer')\n",
    "])\n",
    "\n",
    "DL_model.compile(loss=\"mean_squared_error\",optimizer=tf.keras.optimizers.Adam(0.001)) \n",
    "\n",
    "# define the checkpoint\n",
    "filepath = \"/mnt/ML_HBLUP/NA_RM105_110_115/models/dnn_train_20230516.h5\"\n",
    "checkpoint = keras.callbacks.ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "early_stopping = keras.callbacks.EarlyStopping(patience=8,restore_best_weights=True)\n",
    "callbacks_list = [checkpoint,early_stopping]\n",
    "\n",
    "# fit the model\n",
    "history = DL_model.fit(X_train,y_train,epochs=50,validation_data = (X_val, y_val),callbacks=callbacks_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "fd58ba92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the saved neural network model\n",
    "DL_model = tf.keras.models.load_model('/mnt/ML_HBLUP/NA_RM105_110_115/models/dnn_train_20230516.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "aec11ebc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-24 14:52:08.571119: W tensorflow/core/framework/cpu_allocator_impl.cc:80] Allocation of 21131371184 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Method</th>\n",
       "      <th>RMSE_train</th>\n",
       "      <th>RMSE_val</th>\n",
       "      <th>RMSE_test</th>\n",
       "      <th>corr_train</th>\n",
       "      <th>corr_val</th>\n",
       "      <th>corr_test</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LightGBM_hapCat</td>\n",
       "      <td>2.305420</td>\n",
       "      <td>4.553930</td>\n",
       "      <td>4.440251</td>\n",
       "      <td>0.983027</td>\n",
       "      <td>0.932513</td>\n",
       "      <td>0.933978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DNN_hapCat</td>\n",
       "      <td>3.037271</td>\n",
       "      <td>4.901777</td>\n",
       "      <td>4.718805</td>\n",
       "      <td>0.969738</td>\n",
       "      <td>0.921050</td>\n",
       "      <td>0.925132</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Method  RMSE_train  RMSE_val  RMSE_test  corr_train  corr_val  \\\n",
       "0  LightGBM_hapCat    2.305420  4.553930   4.440251    0.983027  0.932513   \n",
       "0       DNN_hapCat    3.037271  4.901777   4.718805    0.969738  0.921050   \n",
       "\n",
       "   corr_test  \n",
       "0   0.933978  \n",
       "0   0.925132  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# output prediction result\n",
    "metrics = pred_rmse_corr(DL_model,'DNN_hapCat',X_train,y_train,X_val,y_val,X_test,y_test,metrics)\n",
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f37057c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_layer (Flatten)        (None, 109598)            0         \n",
      "_________________________________________________________________\n",
      "hidden_layer1 (Dense)        (None, 100)               10959900  \n",
      "_________________________________________________________________\n",
      "hidden_layer2 (Dense)        (None, 300)               30300     \n",
      "_________________________________________________________________\n",
      "output_layer (Dense)         (None, 1)                 301       \n",
      "=================================================================\n",
      "Total params: 10,990,501\n",
      "Trainable params: 10,990,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "DL_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ab411924",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "pd.DataFrame(history.history).plot(figsize = (8,5))\n",
    "plt.grid(True)\n",
    "plt.gca().set_ylim(0,30)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36749c51",
   "metadata": {},
   "source": [
    "## CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "679d57d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         [(None, 109598, 1)]       0         \n",
      "_________________________________________________________________\n",
      "conv1d_4 (Conv1D)            (None, 109596, 16)        64        \n",
      "_________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1 (None, 27399, 16)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_5 (Conv1D)            (None, 27397, 16)         784       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_5 (MaxPooling1 (None, 6849, 16)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_6 (Conv1D)            (None, 6847, 16)          784       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_6 (MaxPooling1 (None, 1711, 16)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_7 (Conv1D)            (None, 1709, 32)          1568      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_7 (MaxPooling1 (None, 427, 32)           0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 13664)             0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 100)               1366500   \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 1,369,801\n",
      "Trainable params: 1,369,801\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input, Conv1D, MaxPooling1D, Flatten, Dense\n",
    "\n",
    "tf.random.set_seed(20230516)\n",
    "# Define the input and output dimensions\n",
    "input_dim = X_train.shape[1]\n",
    "output_dim = 1\n",
    "\n",
    "# Input layer\n",
    "input_layer = Input(shape=(input_dim, 1))\n",
    "\n",
    "# Layer 1: Convolutional layer\n",
    "conv1 = Conv1D(filters=16, kernel_size=3, activation='relu')(input_layer)\n",
    "pool1 = MaxPooling1D(pool_size=4)(conv1)\n",
    "\n",
    "# Layer 2: Convolutional layer\n",
    "conv2 = Conv1D(filters=16, kernel_size=3, activation='relu')(pool1)\n",
    "pool2 = MaxPooling1D(pool_size=4)(conv2)\n",
    "\n",
    "\n",
    "# Layer 3: Convolutional layer\n",
    "conv3 = Conv1D(filters=16, kernel_size=3, activation='relu')(pool2)\n",
    "pool3 = MaxPooling1D(pool_size=4)(conv3)\n",
    "\n",
    "\n",
    "# Layer 3: Convolutional layer\n",
    "conv4 = Conv1D(filters=32, kernel_size=3, activation='relu')(pool3)\n",
    "pool4 = MaxPooling1D(pool_size=4)(conv4)\n",
    "\n",
    "\n",
    "# Layer 4: Flatten layer and Dense layer\n",
    "flatten = Flatten()(pool4)\n",
    "dense1 = Dense(100, activation='relu')(flatten)\n",
    "# dense2 = Dense(100, activation='relu')(dense1)\n",
    "output_layer = Dense(output_dim)(dense1)\n",
    "\n",
    "# Create the model\n",
    "CNN_model = Model(inputs=input_layer, outputs=output_layer)\n",
    "\n",
    "# # Compile the model with appropriate loss function for regression\n",
    "CNN_model.compile(loss=\"mean_squared_error\",\n",
    "              optimizer=tf.keras.optimizers.Adam(0.001),\n",
    "              metrics=[\"MeanSquaredError\"])\n",
    "# Print the model summary\n",
    "CNN_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ce7a55f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  0\n"
     ]
    }
   ],
   "source": [
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ea20fa25",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-24 14:58:45.702369: W tensorflow/core/framework/cpu_allocator_impl.cc:80] Allocation of 21131371184 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "754/754 [==============================] - 259s 343ms/step - loss: 93.1609 - mean_squared_error: 93.1609 - val_loss: 55.2592 - val_mean_squared_error: 55.2592\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 55.25920, saving model to /mnt/ML_HBLUP/NA_RM105_110_115/models/cnn_train_20230516.h5\n",
      "Epoch 2/50\n",
      "754/754 [==============================] - 256s 340ms/step - loss: 49.9749 - mean_squared_error: 49.9749 - val_loss: 46.5511 - val_mean_squared_error: 46.5511\n",
      "\n",
      "Epoch 00002: val_loss improved from 55.25920 to 46.55114, saving model to /mnt/ML_HBLUP/NA_RM105_110_115/models/cnn_train_20230516.h5\n",
      "Epoch 3/50\n",
      "754/754 [==============================] - 258s 342ms/step - loss: 44.4954 - mean_squared_error: 44.4954 - val_loss: 42.8860 - val_mean_squared_error: 42.8860\n",
      "\n",
      "Epoch 00003: val_loss improved from 46.55114 to 42.88605, saving model to /mnt/ML_HBLUP/NA_RM105_110_115/models/cnn_train_20230516.h5\n",
      "Epoch 4/50\n",
      "754/754 [==============================] - 257s 341ms/step - loss: 38.4136 - mean_squared_error: 38.4136 - val_loss: 40.9064 - val_mean_squared_error: 40.9064\n",
      "\n",
      "Epoch 00004: val_loss improved from 42.88605 to 40.90643, saving model to /mnt/ML_HBLUP/NA_RM105_110_115/models/cnn_train_20230516.h5\n",
      "Epoch 5/50\n",
      "754/754 [==============================] - 258s 343ms/step - loss: 36.2080 - mean_squared_error: 36.2080 - val_loss: 37.4712 - val_mean_squared_error: 37.4712\n",
      "\n",
      "Epoch 00005: val_loss improved from 40.90643 to 37.47117, saving model to /mnt/ML_HBLUP/NA_RM105_110_115/models/cnn_train_20230516.h5\n",
      "Epoch 6/50\n",
      "754/754 [==============================] - 260s 345ms/step - loss: 33.6468 - mean_squared_error: 33.6468 - val_loss: 37.2002 - val_mean_squared_error: 37.2002\n",
      "\n",
      "Epoch 00006: val_loss improved from 37.47117 to 37.20016, saving model to /mnt/ML_HBLUP/NA_RM105_110_115/models/cnn_train_20230516.h5\n",
      "Epoch 7/50\n",
      "754/754 [==============================] - 258s 343ms/step - loss: 31.6541 - mean_squared_error: 31.6541 - val_loss: 37.6307 - val_mean_squared_error: 37.6307\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 37.20016\n",
      "Epoch 8/50\n",
      "754/754 [==============================] - 261s 346ms/step - loss: 28.8544 - mean_squared_error: 28.8544 - val_loss: 34.0055 - val_mean_squared_error: 34.0055\n",
      "\n",
      "Epoch 00008: val_loss improved from 37.20016 to 34.00547, saving model to /mnt/ML_HBLUP/NA_RM105_110_115/models/cnn_train_20230516.h5\n",
      "Epoch 9/50\n",
      "754/754 [==============================] - 260s 345ms/step - loss: 28.0354 - mean_squared_error: 28.0354 - val_loss: 33.5391 - val_mean_squared_error: 33.5391\n",
      "\n",
      "Epoch 00009: val_loss improved from 34.00547 to 33.53907, saving model to /mnt/ML_HBLUP/NA_RM105_110_115/models/cnn_train_20230516.h5\n",
      "Epoch 10/50\n",
      "754/754 [==============================] - 259s 344ms/step - loss: 25.0739 - mean_squared_error: 25.0739 - val_loss: 31.8428 - val_mean_squared_error: 31.8428\n",
      "\n",
      "Epoch 00010: val_loss improved from 33.53907 to 31.84281, saving model to /mnt/ML_HBLUP/NA_RM105_110_115/models/cnn_train_20230516.h5\n",
      "Epoch 11/50\n",
      "754/754 [==============================] - 259s 344ms/step - loss: 24.7138 - mean_squared_error: 24.7138 - val_loss: 31.1887 - val_mean_squared_error: 31.1887\n",
      "\n",
      "Epoch 00011: val_loss improved from 31.84281 to 31.18874, saving model to /mnt/ML_HBLUP/NA_RM105_110_115/models/cnn_train_20230516.h5\n",
      "Epoch 12/50\n",
      "754/754 [==============================] - 260s 345ms/step - loss: 22.5095 - mean_squared_error: 22.5095 - val_loss: 31.2552 - val_mean_squared_error: 31.2552\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 31.18874\n",
      "Epoch 13/50\n",
      "754/754 [==============================] - 260s 344ms/step - loss: 22.3624 - mean_squared_error: 22.3624 - val_loss: 29.4908 - val_mean_squared_error: 29.4908\n",
      "\n",
      "Epoch 00013: val_loss improved from 31.18874 to 29.49083, saving model to /mnt/ML_HBLUP/NA_RM105_110_115/models/cnn_train_20230516.h5\n",
      "Epoch 14/50\n",
      "754/754 [==============================] - 261s 346ms/step - loss: 20.6023 - mean_squared_error: 20.6023 - val_loss: 29.6085 - val_mean_squared_error: 29.6085\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 29.49083\n",
      "Epoch 15/50\n",
      "754/754 [==============================] - 262s 348ms/step - loss: 19.0731 - mean_squared_error: 19.0731 - val_loss: 29.0512 - val_mean_squared_error: 29.0512\n",
      "\n",
      "Epoch 00015: val_loss improved from 29.49083 to 29.05124, saving model to /mnt/ML_HBLUP/NA_RM105_110_115/models/cnn_train_20230516.h5\n",
      "Epoch 16/50\n",
      "627/754 [=======================>......] - ETA: 41s - loss: 17.7632 - mean_squared_error: 17.7632"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "754/754 [==============================] - 265s 352ms/step - loss: 17.8103 - mean_squared_error: 17.8103 - val_loss: 26.6808 - val_mean_squared_error: 26.6808\n",
      "\n",
      "Epoch 00018: val_loss improved from 27.61962 to 26.68076, saving model to /mnt/ML_HBLUP/NA_RM105_110_115/models/cnn_train_20230516.h5\n",
      "Epoch 19/50\n",
      "754/754 [==============================] - 264s 350ms/step - loss: 16.9675 - mean_squared_error: 16.9675 - val_loss: 27.1857 - val_mean_squared_error: 27.1857\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 26.68076\n",
      "Epoch 20/50\n",
      "754/754 [==============================] - 264s 351ms/step - loss: 15.7913 - mean_squared_error: 15.7913 - val_loss: 27.8271 - val_mean_squared_error: 27.8271\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 26.68076\n",
      "Epoch 21/50\n",
      "754/754 [==============================] - 265s 351ms/step - loss: 14.7450 - mean_squared_error: 14.7450 - val_loss: 27.9884 - val_mean_squared_error: 27.9884\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 26.68076\n",
      "Epoch 22/50\n",
      "754/754 [==============================] - 266s 352ms/step - loss: 14.6472 - mean_squared_error: 14.6472 - val_loss: 26.6989 - val_mean_squared_error: 26.6989\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 26.68076\n",
      "Epoch 23/50\n",
      "754/754 [==============================] - 269s 357ms/step - loss: 14.1289 - mean_squared_error: 14.1289 - val_loss: 27.4254 - val_mean_squared_error: 27.4254\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 26.68076\n",
      "Epoch 24/50\n",
      "754/754 [==============================] - 268s 356ms/step - loss: 14.0569 - mean_squared_error: 14.0569 - val_loss: 28.5942 - val_mean_squared_error: 28.5942\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 26.68076\n",
      "Epoch 25/50\n",
      "754/754 [==============================] - 267s 355ms/step - loss: 13.5677 - mean_squared_error: 13.5677 - val_loss: 25.2620 - val_mean_squared_error: 25.2620\n",
      "\n",
      "Epoch 00025: val_loss improved from 26.68076 to 25.26202, saving model to /mnt/ML_HBLUP/NA_RM105_110_115/models/cnn_train_20230516.h5\n",
      "Epoch 26/50\n",
      "754/754 [==============================] - 270s 358ms/step - loss: 12.7590 - mean_squared_error: 12.7590 - val_loss: 25.2774 - val_mean_squared_error: 25.2774\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 25.26202\n",
      "Epoch 27/50\n",
      "754/754 [==============================] - 268s 355ms/step - loss: 11.9658 - mean_squared_error: 11.9658 - val_loss: 25.3833 - val_mean_squared_error: 25.3833\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 25.26202\n",
      "Epoch 28/50\n",
      "754/754 [==============================] - 270s 358ms/step - loss: 11.7107 - mean_squared_error: 11.7107 - val_loss: 25.4583 - val_mean_squared_error: 25.4583\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 25.26202\n",
      "Epoch 29/50\n",
      "754/754 [==============================] - 271s 359ms/step - loss: 11.1893 - mean_squared_error: 11.1893 - val_loss: 25.7704 - val_mean_squared_error: 25.7704\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 25.26202\n",
      "Epoch 30/50\n",
      "754/754 [==============================] - 266s 353ms/step - loss: 11.3413 - mean_squared_error: 11.3413 - val_loss: 25.3730 - val_mean_squared_error: 25.3730\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 25.26202\n",
      "Epoch 31/50\n",
      "754/754 [==============================] - 267s 355ms/step - loss: 10.7934 - mean_squared_error: 10.7934 - val_loss: 24.6591 - val_mean_squared_error: 24.6591\n",
      "\n",
      "Epoch 00031: val_loss improved from 25.26202 to 24.65906, saving model to /mnt/ML_HBLUP/NA_RM105_110_115/models/cnn_train_20230516.h5\n",
      "Epoch 32/50\n",
      "754/754 [==============================] - 266s 353ms/step - loss: 10.0833 - mean_squared_error: 10.0833 - val_loss: 24.7261 - val_mean_squared_error: 24.7261\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 24.65906\n",
      "Epoch 33/50\n",
      "754/754 [==============================] - 269s 357ms/step - loss: 9.3432 - mean_squared_error: 9.3432 - val_loss: 25.1711 - val_mean_squared_error: 25.1711\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 24.56044\n",
      "Epoch 35/50\n",
      "754/754 [==============================] - 268s 355ms/step - loss: 9.0698 - mean_squared_error: 9.0698 - val_loss: 25.1309 - val_mean_squared_error: 25.1309\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 24.56044\n",
      "Epoch 36/50\n",
      "754/754 [==============================] - 267s 354ms/step - loss: 8.5672 - mean_squared_error: 8.5672 - val_loss: 24.7201 - val_mean_squared_error: 24.7201\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 24.56044\n",
      "Epoch 37/50\n",
      "754/754 [==============================] - 268s 356ms/step - loss: 8.4859 - mean_squared_error: 8.4859 - val_loss: 23.8715 - val_mean_squared_error: 23.8715\n",
      "\n",
      "Epoch 00037: val_loss improved from 24.56044 to 23.87150, saving model to /mnt/ML_HBLUP/NA_RM105_110_115/models/cnn_train_20230516.h5\n",
      "Epoch 38/50\n",
      "  4/754 [..............................] - ETA: 4:09 - loss: 7.3156 - mean_squared_error: 7.3156"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "754/754 [==============================] - 267s 354ms/step - loss: 7.9685 - mean_squared_error: 7.9685 - val_loss: 24.8927 - val_mean_squared_error: 24.8927\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 23.87150\n",
      "Epoch 40/50\n",
      "754/754 [==============================] - 267s 354ms/step - loss: 7.3745 - mean_squared_error: 7.3745 - val_loss: 23.8324 - val_mean_squared_error: 23.8324\n",
      "\n",
      "Epoch 00040: val_loss improved from 23.87150 to 23.83238, saving model to /mnt/ML_HBLUP/NA_RM105_110_115/models/cnn_train_20230516.h5\n",
      "Epoch 41/50\n",
      "754/754 [==============================] - 268s 355ms/step - loss: 7.2809 - mean_squared_error: 7.2809 - val_loss: 25.1220 - val_mean_squared_error: 25.1220\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 23.83238\n",
      "Epoch 42/50\n",
      "754/754 [==============================] - 268s 355ms/step - loss: 7.1035 - mean_squared_error: 7.1035 - val_loss: 23.2816 - val_mean_squared_error: 23.2816\n",
      "\n",
      "Epoch 00042: val_loss improved from 23.83238 to 23.28164, saving model to /mnt/ML_HBLUP/NA_RM105_110_115/models/cnn_train_20230516.h5\n",
      "Epoch 43/50\n",
      "754/754 [==============================] - 268s 355ms/step - loss: 7.1466 - mean_squared_error: 7.1466 - val_loss: 23.9272 - val_mean_squared_error: 23.9272\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 23.28164\n",
      "Epoch 44/50\n",
      "754/754 [==============================] - 265s 352ms/step - loss: 6.8667 - mean_squared_error: 6.8667 - val_loss: 24.2097 - val_mean_squared_error: 24.2097\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 23.28164\n",
      "Epoch 45/50\n",
      "754/754 [==============================] - 261s 346ms/step - loss: 6.4117 - mean_squared_error: 6.4117 - val_loss: 24.7694 - val_mean_squared_error: 24.7694\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 23.28164\n",
      "Epoch 46/50\n",
      "754/754 [==============================] - 262s 348ms/step - loss: 6.5502 - mean_squared_error: 6.5502 - val_loss: 23.8145 - val_mean_squared_error: 23.8145\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 23.28164\n",
      "Epoch 47/50\n",
      "754/754 [==============================] - 263s 349ms/step - loss: 6.0333 - mean_squared_error: 6.0333 - val_loss: 23.9963 - val_mean_squared_error: 23.9963\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 23.28164\n",
      "Epoch 48/50\n",
      "754/754 [==============================] - 260s 345ms/step - loss: 5.7701 - mean_squared_error: 5.7701 - val_loss: 24.1035 - val_mean_squared_error: 24.1035\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 23.28164\n",
      "Epoch 49/50\n",
      "754/754 [==============================] - 259s 343ms/step - loss: 5.7315 - mean_squared_error: 5.7315 - val_loss: 24.2012 - val_mean_squared_error: 24.2012\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 23.28164\n",
      "Epoch 50/50\n",
      "754/754 [==============================] - 261s 347ms/step - loss: 5.4685 - mean_squared_error: 5.4685 - val_loss: 24.4160 - val_mean_squared_error: 24.4160\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 23.28164\n"
     ]
    }
   ],
   "source": [
    "# define the checkpoint\n",
    "filepath = \"/mnt/ML_HBLUP/NA_RM105_110_115/models/cnn_train_20230516.h5\"\n",
    "checkpoint = keras.callbacks.ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "early_stopping = keras.callbacks.EarlyStopping(patience=8,restore_best_weights=True)\n",
    "callbacks_list = [checkpoint,early_stopping]\n",
    "\n",
    "# fit the model\n",
    "history = CNN_model.fit(X_train,y_train,epochs=50,validation_data = (X_val, y_val),callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "84a4f9cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-24 18:45:30.833235: W tensorflow/core/framework/cpu_allocator_impl.cc:80] Allocation of 21131371184 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Method</th>\n",
       "      <th>RMSE_train</th>\n",
       "      <th>RMSE_val</th>\n",
       "      <th>RMSE_test</th>\n",
       "      <th>corr_train</th>\n",
       "      <th>corr_val</th>\n",
       "      <th>corr_test</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LightGBM_hapCat</td>\n",
       "      <td>2.305420</td>\n",
       "      <td>4.553930</td>\n",
       "      <td>4.440251</td>\n",
       "      <td>0.983027</td>\n",
       "      <td>0.932513</td>\n",
       "      <td>0.933978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DNN_hapCat</td>\n",
       "      <td>3.037271</td>\n",
       "      <td>4.901777</td>\n",
       "      <td>4.718805</td>\n",
       "      <td>0.969738</td>\n",
       "      <td>0.921050</td>\n",
       "      <td>0.925132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CNN_hapCat</td>\n",
       "      <td>2.431327</td>\n",
       "      <td>4.825105</td>\n",
       "      <td>4.765306</td>\n",
       "      <td>0.981043</td>\n",
       "      <td>0.923493</td>\n",
       "      <td>0.923521</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Method  RMSE_train  RMSE_val  RMSE_test  corr_train  corr_val  \\\n",
       "0  LightGBM_hapCat    2.305420  4.553930   4.440251    0.983027  0.932513   \n",
       "0       DNN_hapCat    3.037271  4.901777   4.718805    0.969738  0.921050   \n",
       "0       CNN_hapCat    2.431327  4.825105   4.765306    0.981043  0.923493   \n",
       "\n",
       "   corr_test  \n",
       "0   0.933978  \n",
       "0   0.925132  \n",
       "0   0.923521  "
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# output prediction result\n",
    "metrics = pred_rmse_corr(CNN_model,'CNN_hapCat',X_train,y_train,X_val,y_val,X_test,y_test,metrics)\n",
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "394e68b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Method</th>\n",
       "      <th>RMSE_train</th>\n",
       "      <th>RMSE_val</th>\n",
       "      <th>RMSE_test</th>\n",
       "      <th>corr_train</th>\n",
       "      <th>corr_val</th>\n",
       "      <th>corr_test</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LightGBM_hapCat</td>\n",
       "      <td>2.305420</td>\n",
       "      <td>4.553930</td>\n",
       "      <td>4.440251</td>\n",
       "      <td>0.983027</td>\n",
       "      <td>0.932513</td>\n",
       "      <td>0.933978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DNN_hapCat</td>\n",
       "      <td>3.037271</td>\n",
       "      <td>4.901777</td>\n",
       "      <td>4.718805</td>\n",
       "      <td>0.969738</td>\n",
       "      <td>0.921050</td>\n",
       "      <td>0.925132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CNN_hapCat</td>\n",
       "      <td>2.431327</td>\n",
       "      <td>4.825105</td>\n",
       "      <td>4.765306</td>\n",
       "      <td>0.981043</td>\n",
       "      <td>0.923493</td>\n",
       "      <td>0.923521</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Method  RMSE_train  RMSE_val  RMSE_test  corr_train  corr_val  \\\n",
       "0  LightGBM_hapCat    2.305420  4.553930   4.440251    0.983027  0.932513   \n",
       "0       DNN_hapCat    3.037271  4.901777   4.718805    0.969738  0.921050   \n",
       "0       CNN_hapCat    2.431327  4.825105   4.765306    0.981043  0.923493   \n",
       "\n",
       "   corr_test  \n",
       "0   0.933978  \n",
       "0   0.925132  \n",
       "0   0.923521  "
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f83edb03",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6049c90",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "86eac843",
   "metadata": {},
   "source": [
    "# try probability model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "455811ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in female data\n",
    "result = pyreadr.read_r('/mnt/ML_HBLUP/NA_RM105_110_115/data/astProb_female.rds') # also works for RData\n",
    "# done! \n",
    "# result is a dictionary where keys are the name of objects and the values python\n",
    "# objects. In the case of Rds there is only one object with None as key\n",
    "femaleData = result[None] # extract the pandas data frame \n",
    "\n",
    "# read in male data\n",
    "result = pyreadr.read_r('/mnt/ML_HBLUP/NA_RM105_110_115/data/astProb_male.rds') # also works for RData\n",
    "maleData = result[None] # extract the pandas data frame "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "1a7cbcf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add suffix\n",
    "femaleData.columns += '_f'\n",
    "maleData.columns += '_m'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f140e0c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct haplotype data for test and train data\n",
    "trainHap = pd.concat([femaleData.loc[trainPheno['FEMALE'],:].reset_index(drop=True),\n",
    "                      maleData.loc[trainPheno['MALE'],:].reset_index(drop=True)],axis=1)\n",
    "trainHap = trainHap / 2\n",
    "\n",
    "testHap = pd.concat([femaleData.loc[testPheno['FEMALE'],:].reset_index(drop=True),\n",
    "                     maleData.loc[testPheno['MALE'],:].reset_index(drop=True)],axis=1)\n",
    "testHap = testHap / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e20df2e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(33475, 109598)\n",
      "(7307, 109598)\n"
     ]
    }
   ],
   "source": [
    "print(trainHap.shape)\n",
    "print(testHap.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "90651417",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>HB1__1067-1_f</th>\n",
       "      <th>HB1__32843_f</th>\n",
       "      <th>HB1__64DWA2_f</th>\n",
       "      <th>HB1__B73_f</th>\n",
       "      <th>HB1__MANS_f</th>\n",
       "      <th>HB1__NA_f</th>\n",
       "      <th>HB1__WDAQ2_f</th>\n",
       "      <th>HB2__1067-1_f</th>\n",
       "      <th>HB2__32843_f</th>\n",
       "      <th>HB2__64DWA2_f</th>\n",
       "      <th>...</th>\n",
       "      <th>HB17114__OH07_m</th>\n",
       "      <th>HB17114__PH207_m</th>\n",
       "      <th>HB17115__LH123_m</th>\n",
       "      <th>HB17115__NA_m</th>\n",
       "      <th>HB17115__OH07_m</th>\n",
       "      <th>HB17115__TA1180_m</th>\n",
       "      <th>HB17116__LH123_m</th>\n",
       "      <th>HB17116__NA_m</th>\n",
       "      <th>HB17116__OH07_m</th>\n",
       "      <th>HB17116__PH207_m</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.49665</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.49975</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.49965</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.48405</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.48175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.46920</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.46515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.49055</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.48965</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 109598 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   HB1__1067-1_f  HB1__32843_f  HB1__64DWA2_f  HB1__B73_f  HB1__MANS_f  \\\n",
       "0            0.0           0.0            0.0     0.49665          0.0   \n",
       "1            0.5           0.0            0.0     0.00000          0.0   \n",
       "2            0.0           0.0            0.0     0.00000          0.0   \n",
       "3            0.0           0.0            0.0     0.00000          0.0   \n",
       "4            0.0           0.0            0.0     0.00000          0.0   \n",
       "\n",
       "   HB1__NA_f  HB1__WDAQ2_f  HB2__1067-1_f  HB2__32843_f  HB2__64DWA2_f  ...  \\\n",
       "0        0.0           0.0            0.0           0.0            0.0  ...   \n",
       "1        0.0           0.0            0.5           0.0            0.0  ...   \n",
       "2        0.5           0.0            0.0           0.0            0.0  ...   \n",
       "3        0.5           0.0            0.0           0.0            0.0  ...   \n",
       "4        0.5           0.0            0.0           0.0            0.0  ...   \n",
       "\n",
       "   HB17114__OH07_m  HB17114__PH207_m  HB17115__LH123_m  HB17115__NA_m  \\\n",
       "0              0.0           0.00000           0.00000        0.49975   \n",
       "1              0.0           0.48405           0.00000        0.00000   \n",
       "2              0.0           0.46920           0.00000        0.00000   \n",
       "3              0.0           0.00000           0.00000        0.00000   \n",
       "4              0.0           0.00000           0.49055        0.00000   \n",
       "\n",
       "   HB17115__OH07_m  HB17115__TA1180_m  HB17116__LH123_m  HB17116__NA_m  \\\n",
       "0              0.0                0.0           0.00000        0.49965   \n",
       "1              0.0                0.0           0.00000        0.00000   \n",
       "2              0.0                0.0           0.00000        0.00000   \n",
       "3              0.0                0.0           0.00000        0.00000   \n",
       "4              0.0                0.0           0.48965        0.00000   \n",
       "\n",
       "   HB17116__OH07_m  HB17116__PH207_m  \n",
       "0              0.0           0.00000  \n",
       "1              0.0           0.48175  \n",
       "2              0.0           0.46515  \n",
       "3              0.0           0.00000  \n",
       "4              0.0           0.00000  \n",
       "\n",
       "[5 rows x 109598 columns]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainHap.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "cea1d50a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# splite the train data into train, validationa and test\n",
    "seed = 20230510\n",
    "np.random.seed(seed)\n",
    "X_train, X_test, y_train, y_test = train_test_split(trainHap, trainPheno['YLD_BE_BLUP'], test_size=0.1, random_state=seed)\n",
    "\n",
    "seed = 20230515\n",
    "np.random.seed(seed)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "16e7140c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(24101, 109598)\n",
      "(6026, 109598)\n",
      "(3348, 109598)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(X_val.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "bd383be7",
   "metadata": {},
   "outputs": [],
   "source": [
    "del trainHap; del result; del femaleData; del maleData"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "107801b4",
   "metadata": {},
   "source": [
    "## HapProb - LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "2f4d2e44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] feature_fraction is set=0.5, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n",
      "Training until validation scores don't improve for 8 rounds\n",
      "Early stopping, best iteration is:\n",
      "[826]\tvalid_0's l2: 19.6989\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LGBMRegressor(bagging_fraction=0.7, feature_fraction=0.5, n_estimators=900,\n",
       "              num_leaves=30, objective='regression')"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gbm = lgb.LGBMRegressor(num_leaves=30,\n",
    "                        learning_rate=0.1,\n",
    "                        n_estimators=900,\n",
    "                       bagging_fraction =  0.7,\n",
    "                       feature_fraction = 0.5,\n",
    "                       objective = \"regression\")\n",
    "\n",
    "gbm.fit(X_train, y_train,\n",
    "        eval_set=[(X_val, y_val)],\n",
    "        eval_metric='mse',\n",
    "        callbacks=[lgb.early_stopping(8)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "1885d10a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Method</th>\n",
       "      <th>RMSE_train</th>\n",
       "      <th>RMSE_val</th>\n",
       "      <th>RMSE_test</th>\n",
       "      <th>corr_train</th>\n",
       "      <th>corr_val</th>\n",
       "      <th>corr_test</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LightGBM_hapCat</td>\n",
       "      <td>2.305420</td>\n",
       "      <td>4.553930</td>\n",
       "      <td>4.440251</td>\n",
       "      <td>0.983027</td>\n",
       "      <td>0.932513</td>\n",
       "      <td>0.933978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DNN_hapCat</td>\n",
       "      <td>3.037271</td>\n",
       "      <td>4.901777</td>\n",
       "      <td>4.718805</td>\n",
       "      <td>0.969738</td>\n",
       "      <td>0.921050</td>\n",
       "      <td>0.925132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CNN_hapCat</td>\n",
       "      <td>2.431327</td>\n",
       "      <td>4.825105</td>\n",
       "      <td>4.765306</td>\n",
       "      <td>0.981043</td>\n",
       "      <td>0.923493</td>\n",
       "      <td>0.923521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LightGBM_hapProb</td>\n",
       "      <td>1.858630</td>\n",
       "      <td>4.438347</td>\n",
       "      <td>4.368822</td>\n",
       "      <td>0.989040</td>\n",
       "      <td>0.935968</td>\n",
       "      <td>0.936030</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Method  RMSE_train  RMSE_val  RMSE_test  corr_train  corr_val  \\\n",
       "0   LightGBM_hapCat    2.305420  4.553930   4.440251    0.983027  0.932513   \n",
       "0        DNN_hapCat    3.037271  4.901777   4.718805    0.969738  0.921050   \n",
       "0        CNN_hapCat    2.431327  4.825105   4.765306    0.981043  0.923493   \n",
       "0  LightGBM_hapProb    1.858630  4.438347   4.368822    0.989040  0.935968   \n",
       "\n",
       "   corr_test  \n",
       "0   0.933978  \n",
       "0   0.925132  \n",
       "0   0.923521  \n",
       "0   0.936030  "
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# output prediction result\n",
    "metrics = pred_rmse_corr(gbm,'LightGBM_hapProb',X_train,y_train,X_val,y_val,X_test,y_test,metrics)\n",
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "53bbf853",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<lightgbm.basic.Booster at 0x7f88e1357b50>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# save model\n",
    "gbm.booster_.save_model('/mnt/ML_HBLUP/NA_RM105_110_115/models/lgbr_hapProb_30_900.txt')\n",
    "\n",
    "# load model later\n",
    "    #model = lightgbm.Booster(model_file='file.txt')\n",
    "    #model.predict(predict[num_columns])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a2834e9",
   "metadata": {},
   "source": [
    "## HapProb - DNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "e68d853c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "754/754 [==============================] - 28s 36ms/step - loss: 65.6610 - val_loss: 43.0543\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 43.05433, saving model to /mnt/ML_HBLUP/NA_RM105_110_115/models/hapProb_dnn_train_20230516.h5\n",
      "Epoch 2/50\n",
      "754/754 [==============================] - 22s 29ms/step - loss: 36.4347 - val_loss: 37.8520\n",
      "\n",
      "Epoch 00002: val_loss improved from 43.05433 to 37.85203, saving model to /mnt/ML_HBLUP/NA_RM105_110_115/models/hapProb_dnn_train_20230516.h5\n",
      "Epoch 3/50\n",
      "754/754 [==============================] - 22s 29ms/step - loss: 32.2113 - val_loss: 33.0789\n",
      "\n",
      "Epoch 00003: val_loss improved from 37.85203 to 33.07889, saving model to /mnt/ML_HBLUP/NA_RM105_110_115/models/hapProb_dnn_train_20230516.h5\n",
      "Epoch 4/50\n",
      "754/754 [==============================] - 22s 29ms/step - loss: 27.5528 - val_loss: 34.2083\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 33.07889\n",
      "Epoch 5/50\n",
      "754/754 [==============================] - 21s 29ms/step - loss: 25.7842 - val_loss: 30.4359\n",
      "\n",
      "Epoch 00005: val_loss improved from 33.07889 to 30.43586, saving model to /mnt/ML_HBLUP/NA_RM105_110_115/models/hapProb_dnn_train_20230516.h5\n",
      "Epoch 6/50\n",
      "754/754 [==============================] - 22s 29ms/step - loss: 23.3681 - val_loss: 30.9434\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 30.43586\n",
      "Epoch 7/50\n",
      "754/754 [==============================] - 22s 29ms/step - loss: 21.8405 - val_loss: 35.1568\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 30.43586\n",
      "Epoch 8/50\n",
      "754/754 [==============================] - 22s 29ms/step - loss: 21.1560 - val_loss: 29.1500\n",
      "\n",
      "Epoch 00008: val_loss improved from 30.43586 to 29.15005, saving model to /mnt/ML_HBLUP/NA_RM105_110_115/models/hapProb_dnn_train_20230516.h5\n",
      "Epoch 9/50\n",
      "754/754 [==============================] - 22s 29ms/step - loss: 19.1031 - val_loss: 27.7298\n",
      "\n",
      "Epoch 00009: val_loss improved from 29.15005 to 27.72981, saving model to /mnt/ML_HBLUP/NA_RM105_110_115/models/hapProb_dnn_train_20230516.h5\n",
      "Epoch 10/50\n",
      "754/754 [==============================] - 22s 29ms/step - loss: 17.8568 - val_loss: 32.6157\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 27.72981\n",
      "Epoch 11/50\n",
      "754/754 [==============================] - 22s 30ms/step - loss: 18.2430 - val_loss: 27.1398\n",
      "\n",
      "Epoch 00011: val_loss improved from 27.72981 to 27.13984, saving model to /mnt/ML_HBLUP/NA_RM105_110_115/models/hapProb_dnn_train_20230516.h5\n",
      "Epoch 12/50\n",
      "754/754 [==============================] - 21s 29ms/step - loss: 16.6319 - val_loss: 26.3054\n",
      "\n",
      "Epoch 00012: val_loss improved from 27.13984 to 26.30536, saving model to /mnt/ML_HBLUP/NA_RM105_110_115/models/hapProb_dnn_train_20230516.h5\n",
      "Epoch 13/50\n",
      "754/754 [==============================] - 22s 29ms/step - loss: 16.5045 - val_loss: 28.8512\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 26.30536\n",
      "Epoch 14/50\n",
      "754/754 [==============================] - 22s 29ms/step - loss: 16.2549 - val_loss: 26.3847\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 26.30536\n",
      "Epoch 15/50\n",
      "754/754 [==============================] - 23s 30ms/step - loss: 15.0216 - val_loss: 26.6868\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 26.30536\n",
      "Epoch 16/50\n",
      "754/754 [==============================] - 23s 30ms/step - loss: 14.0493 - val_loss: 26.2828\n",
      "\n",
      "Epoch 00016: val_loss improved from 26.30536 to 26.28281, saving model to /mnt/ML_HBLUP/NA_RM105_110_115/models/hapProb_dnn_train_20230516.h5\n",
      "Epoch 17/50\n",
      "754/754 [==============================] - 22s 29ms/step - loss: 14.1664 - val_loss: 25.6558\n",
      "\n",
      "Epoch 00017: val_loss improved from 26.28281 to 25.65584, saving model to /mnt/ML_HBLUP/NA_RM105_110_115/models/hapProb_dnn_train_20230516.h5\n",
      "Epoch 18/50\n",
      "754/754 [==============================] - 22s 30ms/step - loss: 14.5369 - val_loss: 24.1579\n",
      "\n",
      "Epoch 00018: val_loss improved from 25.65584 to 24.15786, saving model to /mnt/ML_HBLUP/NA_RM105_110_115/models/hapProb_dnn_train_20230516.h5\n",
      "Epoch 19/50\n",
      "754/754 [==============================] - 22s 30ms/step - loss: 13.7544 - val_loss: 26.8312\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 24.15786\n",
      "Epoch 20/50\n",
      "754/754 [==============================] - 21s 28ms/step - loss: 13.1251 - val_loss: 23.6760\n",
      "\n",
      "Epoch 00020: val_loss improved from 24.15786 to 23.67603, saving model to /mnt/ML_HBLUP/NA_RM105_110_115/models/hapProb_dnn_train_20230516.h5\n",
      "Epoch 21/50\n",
      "754/754 [==============================] - 22s 30ms/step - loss: 12.3619 - val_loss: 25.4547\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 23.67603\n",
      "Epoch 22/50\n",
      "754/754 [==============================] - 22s 30ms/step - loss: 12.9038 - val_loss: 24.9192\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 23.67603\n",
      "Epoch 23/50\n",
      "754/754 [==============================] - 22s 30ms/step - loss: 12.0668 - val_loss: 26.1104\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 23.67603\n",
      "Epoch 24/50\n",
      "754/754 [==============================] - 22s 29ms/step - loss: 12.5071 - val_loss: 24.6213\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 23.67603\n",
      "Epoch 25/50\n",
      "754/754 [==============================] - 22s 29ms/step - loss: 11.5308 - val_loss: 24.4061\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 23.67603\n",
      "Epoch 26/50\n",
      "754/754 [==============================] - 21s 28ms/step - loss: 11.3104 - val_loss: 23.6346\n",
      "\n",
      "Epoch 00026: val_loss improved from 23.67603 to 23.63463, saving model to /mnt/ML_HBLUP/NA_RM105_110_115/models/hapProb_dnn_train_20230516.h5\n",
      "Epoch 27/50\n",
      "754/754 [==============================] - 21s 28ms/step - loss: 11.1710 - val_loss: 26.7232\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 23.63463\n",
      "Epoch 28/50\n",
      "754/754 [==============================] - 22s 29ms/step - loss: 10.5530 - val_loss: 23.9537\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 23.63463\n",
      "Epoch 29/50\n",
      "754/754 [==============================] - 22s 29ms/step - loss: 10.4654 - val_loss: 24.4644\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 23.63463\n",
      "Epoch 30/50\n",
      "754/754 [==============================] - 22s 29ms/step - loss: 11.2475 - val_loss: 23.8623\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 23.63463\n",
      "Epoch 31/50\n",
      "754/754 [==============================] - 21s 28ms/step - loss: 10.2866 - val_loss: 24.4521\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 23.63463\n",
      "Epoch 32/50\n",
      "754/754 [==============================] - 22s 29ms/step - loss: 10.3923 - val_loss: 23.2856\n",
      "\n",
      "Epoch 00032: val_loss improved from 23.63463 to 23.28562, saving model to /mnt/ML_HBLUP/NA_RM105_110_115/models/hapProb_dnn_train_20230516.h5\n",
      "Epoch 33/50\n",
      "754/754 [==============================] - 22s 29ms/step - loss: 10.7392 - val_loss: 23.2867\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 23.28562\n",
      "Epoch 34/50\n",
      "754/754 [==============================] - 22s 29ms/step - loss: 9.6819 - val_loss: 23.3626\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 23.28562\n",
      "Epoch 35/50\n",
      "754/754 [==============================] - 22s 30ms/step - loss: 10.0955 - val_loss: 23.5326\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 23.28562\n",
      "Epoch 36/50\n",
      "754/754 [==============================] - 23s 30ms/step - loss: 9.9755 - val_loss: 23.3228\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 23.28562\n",
      "Epoch 37/50\n",
      "754/754 [==============================] - 22s 29ms/step - loss: 9.7291 - val_loss: 22.9694\n",
      "\n",
      "Epoch 00037: val_loss improved from 23.28562 to 22.96940, saving model to /mnt/ML_HBLUP/NA_RM105_110_115/models/hapProb_dnn_train_20230516.h5\n",
      "Epoch 38/50\n",
      "754/754 [==============================] - 21s 28ms/step - loss: 9.0315 - val_loss: 23.6353\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 22.96940\n",
      "Epoch 39/50\n",
      "754/754 [==============================] - 22s 29ms/step - loss: 9.0465 - val_loss: 22.5249\n",
      "\n",
      "Epoch 00039: val_loss improved from 22.96940 to 22.52486, saving model to /mnt/ML_HBLUP/NA_RM105_110_115/models/hapProb_dnn_train_20230516.h5\n",
      "Epoch 40/50\n",
      "754/754 [==============================] - 22s 29ms/step - loss: 8.8274 - val_loss: 23.0887\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 22.52486\n",
      "Epoch 41/50\n",
      "754/754 [==============================] - 22s 29ms/step - loss: 9.2304 - val_loss: 23.3356\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 22.52486\n",
      "Epoch 42/50\n",
      " 35/754 [>.............................] - ETA: 18s - loss: 8.4524"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tf.random.set_seed(20230516)\n",
    "\n",
    "DL_model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[X_train.shape[1]], name = 'input_layer'),\n",
    "    keras.layers.Dense(100, activation=\"relu\", name = 'hidden_layer1'),\n",
    "    keras.layers.Dense(300, activation=\"relu\", name = 'hidden_layer2'),\n",
    "    keras.layers.Dense(1, name = 'output_layer')\n",
    "])\n",
    "\n",
    "DL_model.compile(loss=\"mean_squared_error\",optimizer=tf.keras.optimizers.Adam(0.001)) \n",
    "\n",
    "# define the checkpoint\n",
    "filepath = \"/mnt/ML_HBLUP/NA_RM105_110_115/models/hapProb_dnn_train_20230516.h5\"\n",
    "checkpoint = keras.callbacks.ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "early_stopping = keras.callbacks.EarlyStopping(patience=8,restore_best_weights=True)\n",
    "callbacks_list = [checkpoint,early_stopping]\n",
    "\n",
    "# fit the model\n",
    "history = DL_model.fit(X_train,y_train,epochs=50,validation_data = (X_val, y_val),callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "346bd1d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Method</th>\n",
       "      <th>RMSE_train</th>\n",
       "      <th>RMSE_val</th>\n",
       "      <th>RMSE_test</th>\n",
       "      <th>corr_train</th>\n",
       "      <th>corr_val</th>\n",
       "      <th>corr_test</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LightGBM_hapCat</td>\n",
       "      <td>2.305420</td>\n",
       "      <td>4.553930</td>\n",
       "      <td>4.440251</td>\n",
       "      <td>0.983027</td>\n",
       "      <td>0.932513</td>\n",
       "      <td>0.933978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DNN_hapCat</td>\n",
       "      <td>3.037271</td>\n",
       "      <td>4.901777</td>\n",
       "      <td>4.718805</td>\n",
       "      <td>0.969738</td>\n",
       "      <td>0.921050</td>\n",
       "      <td>0.925132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CNN_hapCat</td>\n",
       "      <td>2.431327</td>\n",
       "      <td>4.825105</td>\n",
       "      <td>4.765306</td>\n",
       "      <td>0.981043</td>\n",
       "      <td>0.923493</td>\n",
       "      <td>0.923521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LightGBM_hapProb</td>\n",
       "      <td>1.858630</td>\n",
       "      <td>4.438347</td>\n",
       "      <td>4.368822</td>\n",
       "      <td>0.989040</td>\n",
       "      <td>0.935968</td>\n",
       "      <td>0.936030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DNN_hapProb</td>\n",
       "      <td>2.802485</td>\n",
       "      <td>4.746036</td>\n",
       "      <td>4.597196</td>\n",
       "      <td>0.974389</td>\n",
       "      <td>0.926110</td>\n",
       "      <td>0.928817</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Method  RMSE_train  RMSE_val  RMSE_test  corr_train  corr_val  \\\n",
       "0   LightGBM_hapCat    2.305420  4.553930   4.440251    0.983027  0.932513   \n",
       "0        DNN_hapCat    3.037271  4.901777   4.718805    0.969738  0.921050   \n",
       "0        CNN_hapCat    2.431327  4.825105   4.765306    0.981043  0.923493   \n",
       "0  LightGBM_hapProb    1.858630  4.438347   4.368822    0.989040  0.935968   \n",
       "0       DNN_hapProb    2.802485  4.746036   4.597196    0.974389  0.926110   \n",
       "\n",
       "   corr_test  \n",
       "0   0.933978  \n",
       "0   0.925132  \n",
       "0   0.923521  \n",
       "0   0.936030  \n",
       "0   0.928817  "
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# output prediction result\n",
    "metrics = pred_rmse_corr(DL_model,'DNN_hapProb',X_train,y_train,X_val,y_val,X_test,y_test,metrics)\n",
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95463ba4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8bf16c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "be7194b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict 2021 test blup\n",
    "pred_2021 = gbm.predict(testHap)\n",
    "rmse_2021 = mean_squared_error(testPheno['YLD_BE_BLUP'], pred_2021, squared=False)\n",
    "corr_2021, p_value_2021 = pearsonr(testPheno['YLD_BE_BLUP'].ravel(), pred_2021.ravel())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5c7a0679",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18.195141085398273\n",
      "0.10279315746934109\n"
     ]
    }
   ],
   "source": [
    "print(rmse_2021)\n",
    "print(corr_2021)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
